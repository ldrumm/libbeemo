import os
import subprocess

from SCons.Errors import BuildError

Import('env')

"""
The test builders search the tests directory for all C sources, and build each
of them with the current environment, then link them to the libbeemo shared library.
Currently every single C source is assumed to have a main(), so a single test
should be a single file.
If external files need to be built they should be directly #included as
compilable source.
Any extraneous external dependencies not part of the main env() will probably
fail, unless the scons dependency resolver can automatically link the deps as necessary.
This is naturally not a robust solution, but allows us to write quick and simple
tests for the main library.  Hopefully this encourages more sanity-checks.
"""

# __file__ is not set so we need to get the path of this file on import.
# This needs to be done on file import because execute_test() is called from a
# different context with a different PWD.
# This is due to scons building out-of-order (deferred)
__file__ = os.getcwd()


def run_all_tests(env, sources_list, **kwargs):
    """
    In the normal SCons flow, a failing unittest will cause the build to fail
    immediately. We don't want that.  We want a report of all the failures/errors.
    A possible solution would be `scons -i` though it will return success, and
    we need jenkins/travis/whatever to know.
    Also, with `scons -i` the build will return success even if
    multiple failures occur in the main library build.
    This behaviour prevents us from getting the complete picture as we want to
    collect all test results without immediately bailing the build on a test failure.
    Calling this function with a list of sources cause all tests
    to run after a successful build, and pretend the test was successful to prevent
    immediate failure.
    On completion of the test run, the results are inspected and any failing
    testcase  causes a BuildError to be thrown after failures are listed, and the
    failed targets are deleted to

    See http://stackoverflow.com/a/11492516 (and comments) for initial inspiration.
    """

    errors = list()
    failures = list()
    programs = list()
    successes = list()
    executed = list()

    def _report_test_status(target, source, env):
        for test in failures:
            print("=" * 80)
            print("FAILURE: '%s'" % test.test_path)
            print(test.stdout.read())
            print(test.stderr.read())
            print('-' * 80)
        for test in errors:
            print("=" * 80)
            print("ERROR: '%s'" % test.test_path)
            print(test.stdout.read())
            print(test.stderr.read())
            print('-' * 80)
        if any(failures) or any(errors):

            import itertools
            nodes = itertools.chain(
                (failure.target[0] for failure in failures),
                (error.target[0] for error in errors)
            )
            # Hax - The most obvious way to mark the build as dirty is
            # to delete it.
            [x.remove() for x in nodes]
            raise BuildError(
                errstr=(
                    "\n%d test failures%s"
                    "\n%d test errors%s\n") % (
                    len(failures),
                    len(failures) and ": ('%s')" % "', '".join(os.path.basename(test.test_path) for test in failures) or '',
                    len(errors),
                    len(errors) and ": ('%s')" % "', '".join(os.path.basename(test.test_path) for test in errors) or '',
                ),
                node = nodes,
            )
        return 0

    def _execute_test(target, source, env):
        #we want our tests run with a PWD of their source.  This requires hax.
        path = target[0].abspath

        try:
            ps = subprocess.Popen(path,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                shell=False,
                cwd=__file__
            )
            #more hax to allow us to report where the test came from later
            ps.test_path = path
            ps.target = target
            ret = ps.wait()
            if ret == 0:
                successes.append(ps)
            elif ret >= 125:
                #This is likely to be posix (or even bash)-specific
                #https://en.wikipedia.org/wiki/Exit_status
                errors.append(ps)
            else:
                failures.append(ps)
        except OSError as e:
            errors.append(path, e)
        finally:
        # Pretend to return success, whatever the real result.
        # We defer failure notification to the end
            executed.append(ps)
            return 0

    file_extn = kwargs.pop('file_extn', '.c')
    for source in sources_list:
        test = env.Program('test_' + str(source).rstrip(file_extn), [source], **kwargs)
        programs.append(test)
        env.AddPostAction(test,  _execute_test)

    # this is a big hack: In order to allow the scons dependency resolver to work
    # properly, we  must allow out-of-order builds.
    # We must register `_report_test_status` to depend on built tests, and it must
    # check if not all tests have been completed and exit immediately in that case.
    # Because 'results.txt' is never created, this test always runs.
    report_test_status = Command('results.txt', programs, _report_test_status)
    Depends(report_test_status, programs)

    return programs

if os.name == 'nt':
    LIBS = ['beemo', 'm', 'pthread', 'lua', 'portaudio']
else:
    LIBS = ['beemo', 'm', 'pthread', 'lua5.2', 'portaudio']

env.AddMethod(run_all_tests, "RunAllTests")
all_my_tests = env.RunAllTests(Glob('*.c'),  LIBPATH='../', LIBS=LIBS, file_extn='.c')
Return("all_my_tests")
