from __future__ import print_function

import os
import subprocess
import itertools
import uuid

from SCons.Errors import BuildError

Import('env')
env.SConscript('interactive/SConscript', 'env')

"""
The test builders search the tests directory for all C sources, and build each
of them with the current environment, then link them to the libbeemo shared
library. Currently every single C source is assumed to have a main(), so a
single test should be a single file. If external files need to be built they
should be directly #included as compilable source. Any extraneous external
dependencies not part of the main env() will probably fail, unless the scons
dependency resolver can automatically link the deps as necessary. This is
naturally not a robust solution, but allows us to write quick and simple tests
for the main library. Hopefully this encourages more sanity-checks.
"""

# __file__ is not set so we need to get the path of this file on import.
# This needs to be done on file import because execute_test() is called from a
# different context with a different PWD.
# This is due to scons building out-of-order (deferred)
DIRNAME = os.getcwd()


def _print(*args):
    if GetOption('silent'):
        return
    print(*args)


def run_all_tests(env, sources_list, **kwargs):
    """
    In the normal SCons flow, a failing unittest will cause the build to fail
    immediately. We don't want that. We want a report of all the
    failures/errors. A possible solution would be `scons -i` though it will
    return success, and we need jenkins/travis/whatever to know. Also, with
    `scons -i` the build will return success even if multiple failures occur in
    the main library build. This behaviour prevents us from getting the
    complete picture as we want to collect all test results without immediately
    bailing the build on a test failure. Calling this function with a list of
    sources cause all tests to run after a successful build, and pretend the
    test was successful to prevent immediate failure. On completion of the
    test run, the results are inspected and any failing testcase  causes a
    BuildError to be thrown after failures are listed, and the failed targets
    are deleted to force the scons dependency resolver to rebuild on the next
    run.

    See https://stackoverflow.com/a/11492516 (and comments) for initial
    inspiration.
    """

    errors = list()
    failures = list()
    programs = list()
    successes = list()
    executed = list()
    # The extra commands can be used to put the test through extra paces
    # (valgrind much?) expected parameters are the command-line and any options
    # with the '$' dollar character in place of the test path.
    extra_commands = kwargs.pop("extra_commands", list())
    for x in extra_commands:
        assert x.count("$") == 1

    def report_test_status(target, source, env):
        for test in failures:
            _print("=" * 80)
            _print("FAILURE: '%s'" % test.test_path)
            _print(test.stdout.read())
            _print(test.stderr.read())
            _print('-' * 80)
        for test in errors:
            _print("=" * 80)
            _print("ERROR: '%s'" % test.test_path)
            _print(test.stdout.read())
            _print(test.stderr.read())
            _print('-' * 80)
        if any(failures) or any(errors):
            nodes = itertools.chain(
                (failure.target[0] for failure in failures),
                (error.target[0] for error in errors)
            )
            # Hax - The most obvious way to mark the build as dirty is
            # to delete it.
            for x in nodes:
                x.remove()

            raise BuildError(
                errstr=(
                    "\n%d test failures%s"
                    "\n%d test errors%s\n") % (
                        len(failures),
                        failures and ": ('%s')" % "', '".join(
                            os.path.basename(test.test_path) for test in failures
                        ) or '',
                        len(errors),
                        errors and ": ('%s')" % "', '".join(
                            os.path.basename(test.test_path) for test in errors
                        ) or '',
                ),
                node=nodes,
            )
        return 0

    def execute_test(target, source, env):
        # We want our tests run with a PWD of their source; hax.
        path = target[0].abspath
        commands = [path] + list(
            x.replace('$', path).split()
            for x in extra_commands
        )
        for command in commands:
            _print("running %s\n" % command)
            try:
                ps = subprocess.Popen(
                    command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    shell=False,
                    cwd=DIRNAME
                )
                # more hax to allow us to report where the test came from later
                ps.test_path = path
                ps.target = target
                ret = ps.wait()
                if ret == 0:
                    successes.append(ps)
                elif ret >= 125:
                    # This is likely to be posix (or even bash)-specific
                    # https://en.wikipedia.org/wiki/Exit_status
                    errors.append(ps)
                else:
                    failures.append(ps)
            except OSError as e:
                _print(e)
                errors.append(ps)
            finally:
                # Pretend to return success, whatever the real result.
                # We defer failure notification to the end
                executed.append(ps)
        return 0

    file_extn = kwargs.pop('file_extn', '.c')

    for source in sources_list:
        test = env.Program(
            'test_' + str(source).rstrip(file_extn),
            [source],
            **kwargs
        )
        programs.append(test)
        env.AddPostAction(test, execute_test)

    # this is a big hack: In order to allow the scons dependency resolver to
    # work properly, we  must allow out-of-order builds. We must register
    # `report_test_status` to depend on built tests, and it must check if not
    # all tests have been completed and exit immediately in that case. Because
    # the target is always unique and never exists, this test always runs.
    Depends(Command(uuid.uuid4().hex, programs, report_test_status), programs)

    return programs

env.AddMethod(run_all_tests, "RunAllTests")

extra_commands = list()
#if we're running an automated build, we can afford to wait for valgrind:
if any(map(
        lambda x: x in (
            'CI',
            'CONTINUOUS_INTEGRATION',
            'JENKINS',
            'TRAVIS',
            'MEMCHECK'
        ),
        os.environ.keys()
)):
    extra_commands = [
        ('valgrind --error-exitcode=1 --leak-check=full --show-reachable=yes'
            ' --track-origins=yes --num-callers=50 $')
    ]

# We don't want NDEBUG for the tests
# replace the original construction environment
env = env.Clone()
env.Append(LIBS=['beemo', 'm'])
env.Append(LIBPATH='../')
try:
    env['CPPDEFINES'].remove('NDEBUG')
except (ValueError, KeyError):
    pass

all_my_tests = env.RunAllTests(
    Glob('*.c'),
    extra_commands=extra_commands
)

Return("all_my_tests")
